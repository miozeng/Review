# MQ
MQ是消息中间件，是一种跨进程的通信机制，用于上下游传递消息。它能够很好的解除发布订阅者之间的耦合，它将上下游的消息投递解耦成两个部分

### 常见MQ对比
RabbitMQ               
是使用Erlang编写的一个开源的消息队列，本身支持很多的协议：AMQP，XMPP, SMTP, STOMP，也正是如此，使的它变的非常重量级，更适合于企业级的开发。同时实现了一个经纪人(Broker)构架，这意味着消息在发送给客户端时先在中心队列排队。对路由(Routing)，负载均衡(Load balance)或者数据持久化都有很好的支持。

ZeroMQ                 
号称最快的消息队列系统，尤其针对大吞吐量的需求场景。ZMQ能够实现RabbitMQ不擅长的高级/复杂的队列，但是开发人员需要自己组合多种技术框架，技术上的复杂度是对这MQ能够应用成功的挑战。ZeroMQ具有一个独特的非中间件的模式，你不需要安装和运行一个消息服务器或中间件，因为你的应用程序将扮演了这个服务角色。你只需要简单的引用ZeroMQ程序库，可以使用NuGet安装，然后你就可以愉快的在应用程序之间发送消息了。但是ZeroMQ仅提供非持久性的队列，也就是说如果down机，数据将会丢失。其中，Twitter的Storm中使用ZeroMQ作为数据流的传输。                   

ActiveMQ
是Apache下的一个子项目。 类似于ZeroMQ，它能够以代理人和点对点的技术实现队列。同时类似于RabbitMQ，它少量代码就可以高效地实现高级应用场景。RabbitMQ、ZeroMQ、ActiveMQ均支持常用的多种语言客户端 C++、Java、.Net,、Python、 Php、 Ruby等。              

Jafka/Kafka                    
Kafka是Apache下的一个子项目，是一个高性能跨语言分布式Publish/Subscribe消息队列系统，而Jafka是在Kafka之上孵化而来的，即Kafka的一个升级版。具有以下特性：快速持久化，可以在O(1)的系统开销下进行消息持久化；高吞吐，在一台普通的服务器上既可以达到10W/s的吞吐速率；完全的分布式系统，Broker、Producer、Consumer都原生自动支持分布式，自动实现复杂均衡；支持Hadoop数据并行加载，对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka通过Hadoop的并行加载机制来统一了在线和离线的消息处理，这一点也是本课题所研究系统所看重的。Apache Kafka相对于ActiveMQ是一个非常轻量级的消息系统，除了性能非常好之外，还是一个工作良好的分布式系统。                 

### 消息必达
MQ要想尽量消息必达，架构上有两个核心设计点：  
（1）消息落地  
（2）消息超时、重传、确认  
![image](https://github.com/miozeng/Review/blob/master/MQ%E4%B8%8E%E5%90%84%E7%A7%8D%E5%8D%8F%E8%AE%AE/MQflow.png)

MQ消息投递上半场，MQ-client-sender到MQ-server流程见上图1-3：
（1）MQ-client将消息发送给MQ-server（此时业务方调用的是API：SendMsg）
（2）MQ-server将消息落地，落地后即为发送成功
（3）MQ-server将应答发送给MQ-client（此时回调业务方是API：SendCallback）
 
MQ消息投递下半场，MQ-server到MQ-client-receiver流程见上图4-6：
（1）MQ-server将消息发送给MQ-client（此时回调业务方是API：RecvCallback）
（2）MQ-client回复应答给MQ-server（此时业务方主动调用API：SendAck）
（3）MQ-server收到ack，将之前已经落地的消息删除，完成消息的可靠投递

上半场的超时与重传
MQ上半场的1或者2或者3如果丢失或者超时，MQ-client-sender内的timer会重发消息，直到期望收到3，如果重传N次后还未收到，则SendCallback回调发送失败，需要注意的是，这个过程中MQ-server可能会收到同一条消息的多次重发。
 
下半场的超时与重传
MQ下半场的4或者5或者6如果丢失或者超时，MQ-server内的timer会重发消息，直到收到5并且成功执行6，这个过程可能会重发很多次消息，一般采用指数退避的策略，先隔x秒重发，2x秒重发，4x秒重发，以此类推，需要注意的是，这个过程中MQ-client-receiver也可能会收到同一条消息的多次重发。
 

### 消息总线保证幂等
为保证消息的可达性，超时、重传、确认机制可能导致消息总线、或者业务方收到重复的消息，从而对业务产生影响。  
  
上半场的幂等性设计   
1，发送端MQ-client将消息发给服务端MQ-server   
2，服务端MQ-server将消息落地  
3，服务端MQ-server回ACK给发送端MQ-client  
如果3丢失，发送端MQ-client超时后会重发消息，可能导致服务端MQ-server收到重复消息。    
 
此时重发是MQ-client发起的，消息的处理是MQ-server，为了避免步骤2落地重复的消息，对每条消息，MQ系统内部必须生成一个inner-msg-id，作为去重和幂等的依据，这个内部消息ID的特性是：   
（1）全局唯一  
（2）MQ生成，具备业务无关性，对消息发送方和消息接收方屏蔽   
 
有了这个inner-msg-id，就能保证上半场重发，也只有1条消息落到MQ-server的DB中，实现上半场幂等。    

下半场的幂等性设计    
4，服务端MQ-server将消息发给接收端MQ-client   
5，接收端MQ-client回ACK给服务端    
6，服务端MQ-server将落地消息删除   
需要强调的是，接收端MQ-client回ACK给服务端MQ-server，是消息消费业务方的主动调用行为，不能由MQ-client自动发起，因为MQ系统不知道消费方什么时候真正消费成功。   
如果5丢失，服务端MQ-server超时后会重发消息，可能导致MQ-client收到重复的消息。   
 
此时重发是MQ-server发起的，消息的处理是消息消费业务方，消息重发势必导致业务方重复消费（上例中的一次付款，重复发卡），为了保证业务幂等性，业务消息体中，必须有一个biz-id，作为去重和幂等的依据，这个业务ID的特性是：
（1）对于同一个业务场景，全局唯一  
（2）由业务消息发送方生成，业务相关，对MQ透明  
（3）由业务消息消费方负责判重，以保证幂等  
 
最常见的业务ID有：支付ID，订单ID，帖子ID等。  
 

幂等性，不仅对MQ有要求，对业务上下游也有要求。


### 什么时候使用MQ
MQ的不足是：  
1）系统更复杂，多了一个MQ组件   
2）消息传递路径更长，延时会增加  
3）消息可靠性和重复性互为矛盾，消息不丢不重难以同时保证  
4）上游无法知道下游的执行结果，这一点是很致命的   
 
举个栗子：用户登录场景，登录页面调用passport服务，passport服务的执行结果直接影响登录结果，此处的“登录页面”与“passport服务”就必须使用调用关系，而不能使用MQ通信。   
 
无论如何，记住这个结论：调用方实时依赖执行结果的业务场景，请使用调用，而不是MQ。   

什么时候使用MQ   
【典型场景一：数据驱动的任务依赖】   
 什么是任务依赖，举个栗子，互联网公司经常在凌晨进行一些数据统计任务，这些任务之间有一定的依赖关系，比如：  
1）task3需要使用task2的输出作为输入   
2）task2需要使用task1的输出作为输入  
这样的话，tast1, task2, task3之间就有任务依赖关系，必须task1先执行，再task2执行，载task3执行。  
对于这类需求，常见的实现方式是，使用cron人工排执行时间表：
1）task1，0:00执行，经验执行时间为50分钟
2）task2，1:00执行（为task1预留10分钟buffer），经验执行时间也是50分钟
3）task3，2:00执行（为task2预留10分钟buffer）
 
这种方法的坏处是：
1）如果有一个任务执行时间超过了预留buffer的时间，将会得到错误的结果，因为后置任务不清楚前置任务是否执行成功，此时要手动重跑任务，还有可能要调整排班表
2）总任务的执行时间很长，总是要预留很多buffer，如果前置任务提前完成，后置任务不会提前开始
3）如果一个任务被多个任务依赖，这个任务将会称为关键路径，排班表很难体现依赖关系，容易出错
4）如果有一个任务的执行时间要调整，将会有多个任务的执行时间要调整

优化方案是，采用MQ解耦：
1）task1准时开始，结束后发一个“task1 done”的消息
2）task2订阅“task1 done”的消息，收到消息后第一时间启动执行，结束后发一个“task2 done”的消息
3）task3同理
 
采用MQ的优点是：
1）不需要预留buffer，上游任务执行完，下游任务总会在第一时间被执行
2）依赖多个任务，被多个任务依赖都很好处理，只需要订阅相关消息即可
3）有任务执行时间变化，下游任务都不需要调整执行时间
 
需要特别说明的是，MQ只用来传递上游任务执行完成的消息，并不用于传递真正的输入输出数据。


【典型场景二：上游不关心执行结果】
上游需要关注执行结果时要用“调用”，上游不关注执行结果时，就可以使用MQ了。
 
举个栗子，58同城的很多下游需要关注“用户发布帖子”这个事件，比如招聘用户发布帖子后，招聘业务要奖励58豆，房产用户发布帖子后，房产业务要送2个置顶，二手用户发布帖子后，二手业务要修改用户统计数据。

对于这类需求，常见的实现方式是，使用调用关系：
帖子发布服务执行完成之后，调用下游招聘业务、房产业务、二手业务，来完成消息的通知，但事实上，这个通知是否正常正确的执行，帖子发布服务根本不关注。
 
这种方法的坏处是：
1）帖子发布流程的执行时间增加了
2）下游服务当机，可能导致帖子发布服务受影响，上下游逻辑+物理依赖严重
3）每当增加一个需要知道“帖子发布成功”信息的下游，修改代码的是帖子发布服务，这一点是最恶心的，属于架构设计中典型的依赖倒转，谁用过谁痛谁知道（采用此法的请评论留言）
  
 优化方案是，采用MQ解耦：
1）帖子发布成功后，向MQ发一个消息
2）哪个下游关注“帖子发布成功”的消息，主动去MQ订阅
 
采用MQ的优点是：
1）上游执行时间短
2）上下游逻辑+物理解耦，除了与MQ有物理连接，模块之间都不相互依赖
3）新增一个下游消息关注方，上游不需要修改任何代码


【典型场景三：上游关注执行结果，但执行时间很长】
 有时候上游需要关注执行结果，但执行结果时间很长（典型的是调用离线处理，或者跨公网调用），也经常使用回调网关+MQ来解耦。
 
举个栗子，微信支付，跨公网调用微信的接口，执行时间会比较长，但调用方又非常关注执行结果，此时一般怎么玩呢？
![image](https://github.com/miozeng/Review/blob/master/MQ%E4%B8%8E%E5%90%84%E7%A7%8D%E5%8D%8F%E8%AE%AE/wechat.png)
一般采用“回调网关+MQ”方案来解耦：
1）调用方直接跨公网调用微信接口
2）微信返回调用成功，此时并不代表返回成功
3）微信执行完成后，回调统一网关
4）网关将返回结果通知MQ
5）请求方收到结果通知
 
这里需要注意的是，不应该由回调网关来调用上游来通知结果，如果是这样的话，每次新增调用方，回调网关都需要修改代码，仍然会反向依赖，使用回调网关+MQ的方案，新增任何对微信支付的调用，都不需要修改代码啦。

总结：
什么时候不使用MQ？  
上游实时关注执行结果   

什么时候使用MQ？  
1）数据驱动的任务依赖  
2）上游不关心多下游执行结果  
3）异步返回执行时间长  

### 环形队列法
这个跟MQ没有什么关系，但是我也不知道放在哪里去讨论  
很多时候，业务有定时任务或者定时超时的需求，当任务量很大时，可能需要维护大量的timer，或者进行低效的扫描。  
一般来说怎么实现这类需求呢？

“轮询扫描法”
1）用一个Map<uid, last_packet_time>来记录每一个uid最近一次请求时间last_packet_time
2）当某个用户uid有请求包来到，实时更新这个Map
3）启动一个timer，当Map中不为空时，轮询扫描这个Map，看每个uid的last_packet_time是否超过30s，如果超过则进行超时处理
 
“多timer触发法”
1）用一个Map<uid, last_packet_time>来记录每一个uid最近一次请求时间last_packet_time
2）当某个用户uid有请求包来到，实时更新这个Map，并同时对这个uid请求包启动一个timer，30s之后触发
3）每个uid请求包对应的timer触发后，看Map中，查看这个uid的last_packet_time是否超过30s，如果超过则进行超时处理
 
方案一：只启动一个timer，但需要轮询，效率较低
方案二：不需要轮询，但每个请求包要启动一个timer，比较耗资源
 
特别在同时在线量很大时，很容易CPU100%，如何高效维护和触发大量的定时/超时任务，是本文要讨论的问题。
 
二、环形队列法   
废话不多说，三个重要的数据结构：   
1）30s超时，就创建一个index从0到30的环形队列（本质是个数组）  
2）环上每一个slot是一个Set<uid>，任务集合  
3）同时还有一个Map<uid, index>，记录uid落在环上的哪个slot里   

 
同时：   
1）启动一个timer，每隔1s，在上述环形队列中移动一格，0->1->2->3…->29->30->0…  
2）有一个Current Index指针来标识刚检测过的slot   
 
当有某用户uid有请求包到达时：  
1）从Map结构中，查找出这个uid存储在哪一个slot里  
2）从这个slot的Set结构中，删除这个uid  
3）将uid重新加入到新的slot中，具体是哪一个slot呢 => Current Index指针所指向的上一个slot，因为这个slot，会被timer在30s之后扫描到   
4）更新Map，这个uid对应slot的index值    
 
哪些元素会被超时掉呢？
Current Index每秒种移动一个slot，这个slot对应的Set<uid>中所有uid都应该被集体超时！如果最近30s有请求包来到，一定被放到Current Index的前一个slot了，Current Index所在的slot对应Set中所有元素，都是最近30s没有请求包来到的。
 
所以，当没有超时时，Current Index扫到的每一个slot的Set中应该都没有元素。
 
优势：
（1）只需要1个timer
（2）timer每1s只需要一次触发，消耗CPU很低
（3）批量超时，Current Index扫到的slot，Set中所有元素都应该被超时掉
 

   
