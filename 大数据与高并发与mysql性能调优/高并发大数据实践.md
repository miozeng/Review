# 高并发大数据实践
http://www.cnblogs.com/wangchaozhi/p/5061378.html
### 数据库结构的设计

1、数据行的长度不要超过8020字节，如果超过这个长度的话在物理页中这条数据会占用两行从而造成存储碎片，降低查询效率。     

2、能够用数字类型的字段尽量选择数字类型而不用字符串类型的，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。     

3、对于不可变字符类型char和可变字符类型varchar 都是8000字节,char查询快，但是耗存储空间，varchar查询相对慢一些但是节省存储空间。在设计字段的时候可以灵活选择，例如用户名、密码等长度变化不大的字段可以选择CHAR，对于评论等长度变化大的字段可以选择VARCHAR。    

4、字段的长度在最大限度的满足可能的需要的前提下，应该尽可能的设得短一些，这样可以提高查询的效率，而且在建立索引的时候也可以减少资源的消耗。      

### 查询的优化 
保证在实现功能的基础上，尽量减少对数据库的访问次数(可以用缓存保存查询结果，减少查询次数)；              
通过搜索参数，尽量减少对表的访问行数,最小化结果集，从而减轻网络负担；能够分开的操作尽量分开处理，提高每次的响应速度；                
在数据窗口使用SQL时，尽量把使用的索引放在选择的首列；                     
算法的结构尽量简单；                   
在查询时，不要过多地使用通配符如SELECT * FROM T1语句，要用到几列就选择几列如：SELECTCOL1,COL2 FROM T1；                    
在可能的情况下尽量限制尽量结果集行数如：SELECT TOP 300 COL1,COL2,COL3 FROM T1,因为某些情况下用户是不需要那么多的数据的。                

在没有建索引的情况下，数据库查找某一条数据，就必须进行全表扫描了，对所有数据进行一次遍历，查找出符合条件的记录。在数据量比较小的情况下，也许看不出明显的差别，但是当数据量大的情况下，这种情况就是极为糟糕的了。                          


尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。                

避免频繁创建和删除临时表，以减少系统表资源的消耗。                 
   
临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。      
     
在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。   

如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。    

在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。    

尽量避免大事务操作，提高系统并发能力。     

尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。     

避免使用不兼容的数据类型。例如float和int、char和varchar、binary和varbinary是不兼容的（条件判断时）。数据类型的不兼容可能使优化器无法执行一些本来可以进行的优化操作。例如:     

SELECT name FROM employee WHERE salary > 60000     

在这条语句中,如salary字段是money型的,则优化器很难对其进行优化,因为60000是个整型数。我们应当在编程时将整型转化成为钱币型,而不要等到运行时转化。   

充分利用连接条件（条件越多越快），在某种情况下，两个表之间可能不只一个的连接条件，这时在 WHERE 子句中将连接条件完整的写上，有可能大大提高查询速度。   

例：      

SELECT SUM(A.AMOUNT) FROM ACCOUNT A,CARD B WHERE A.CARD_NO = B.CARD_NO    

SELECT SUM(A.AMOUNT) FROM ACCOUNT A,CARD B WHERE A.CARD_NO = B.CARD_NO AND A.ACCOUNT_NO=B.ACCOUNT_NO    

第二句将比第一句执行快得多。     

使用视图加速查询     

把表的一个子集进行排序并创建视图，有时能加速查询。它有助于避免多重排序 操作，而且在其他方面还能简化优化器的工作。例如：    
``` sql
SELECT cust.name，rcvbles.balance，……other columns     

FROM cust，rcvbles     

WHERE cust.customer_id = rcvlbes.customer_id      

AND rcvblls.balance>0     

AND cust.postcode>“98000”     

ORDER BY cust.name    
```
如果这个查询要被执行多次而不止一次，可以把所有未付款的客户找出来放在一个视图中，并按客户的名字进行排序：     
``` sql
CREATE VIEW DBO.V_CUST_RCVLBES    

AS     
 
SELECT cust.name，rcvbles.balance，……other columns     

FROM cust，rcvbles      

WHERE cust.customer_id = rcvlbes.customer_id      

AND rcvblls.balance>0     

ORDER BY cust.name    
```

然后以下面的方式在视图中查询：    

SELECT ＊ FROM V_CUST_RCVLBES      

WHERE postcode>“98000”     

视图中的行要比主表中的行少，而且物理顺序就是所要求的顺序，减少了磁盘I/O，所以查询工作量可以得到大幅减少。     

能用DISTINCT的就不用GROUP BY （group by 操作特别慢）    

SELECT OrderID FROM Details WHERE UnitPrice > 10 GROUP BY OrderID     

可改为：    

SELECT DISTINCT OrderID FROM Details WHERE UnitPrice > 10         

尽量不要用SELECT INTO语句。  

SELECT INOT 语句会导致表锁定，阻止其他用户访问该表。   

上面我们提到的是一些基本的提高查询速度的注意事项,但是在更多的情况下,往往需要反复试验比较不同的语句以得到最佳方案。最好的方法当然是测试，看实现相同功能的SQL语句哪个执行时间最少，但是数据库中如果数据量很少，是比较不出来的，这时可以用查看执行计划，即：把实现相同功能的多条SQL语句考到查询分析器，按CTRL+L看查所利用的索引，表扫描次数（这两个对性能影响最大），总体上看询成本百分比即可。

其他查询优化请参考[sql优化](sql优化.md)   

### 算法的优化

尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。                    

游标提供了对特定集合中逐行扫描的手段，一般使用游标逐行遍历数据，根据取出的数据不同条件进行不同的操作。尤其对多表和大表定义的游标（大的数据集合）循环很容易使程序进入一个漫长的等特甚至死机。                     


### 建立高效的索引      

创建索引一般有以下两个目的：维护被索引列的唯一性和提供快速访问表中数据的策略。大型数据库有两种索引即簇索引和非簇索引，一个没有簇索引的表是按堆结构存储数据，所有的数据均添加在表的尾部，而建立了簇索引的表，其数据在物理上会按照簇索引键的顺序存储，一个表只允许有一个簇索引，因此，根据B树结构，可以理解添加任何一种索引均能提高按索引列查询的速度，但会降低插入、更新、删除操作的性能，尤其是当填充因子（Fill Factor）较大时。所以对索引较多的表进行频繁的插入、更新、删除操作，建表和索引时因设置较小的填充因子，以便在各数据页中留下较多的自由空间，减少页分割及重新组织的工作。      

索引是从数据库中获取数据的最高效方式之一。95% 的数据库性能问题都可以采用索引技术得到解决。作为一条规则，我通常对逻辑主键使用唯一的成组索引，对系统键（作为存储过程）采用唯一的非成组索引，对任何外键列[字段]采用非成组索引。不过，索引就象是盐，太多了菜就咸了。你得考虑数据库的空间有多大，表如何进行访问，还有这些访问是否主要用作读写。      

聚集索引     
　　一种索引，该索引中键值的逻辑顺序决定了表中相应行的物理顺序。      
　　聚集索引确定表中数据的物理顺序。聚集索引类似于电话簿，后者按姓氏排列数据。由于聚集索引规定数据在表中的物理存储顺序，因此一个表只能包含一个聚集索引。但该索引可以包含多个列（组合索引），就像电话簿按姓氏和名字进行组织一样。        
　　聚集索引对于那些经常要搜索范围值的列特别有效。使用聚集索引找到包含第一个值的行后，便可以确保包含后续索引值的行在物理相邻。例如，如果应用程序执行 的一个查询经常检索某一日期范围内的记录，则使用聚集索引可以迅速找到包含开始日期的行，然后检索表中所有相邻的行，直到到达结束日期。这样有助于提高此 类查询的性能。同样，如果对从表中检索的数据进行排序时经常要用到某一列，则可以将该表在该列上聚集（物理排序），避免每次查询该列时都进行排序，从而节 省成本。 
　　 当索引值唯一时，使用聚集索引查找特定的行也很有效率。例如，使用唯一雇员 ID 列 emp_id 查找特定雇员的最快速的方法，是在 emp_id 列上创建聚集索引或 PRIMARY KEY 约束。       

非聚集索引
　　一种索引，该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同。    
    索引是通过二叉树的数据结构来描述的，我们可以这么理解聚簇索引：索引的叶节点就是数据节点。而非聚簇索引的叶节点仍然是索引节点，只不过有一个指针指向对应的数据块。    
    
我们可以很容易的理解：每个表只能有一个聚集索引，因为目录只能按照一种方法进行排序。    

（一）何时使用聚集索引或非聚集索引   

下面的表总结了何时使用聚集索引或非聚集索引（很重要）。   

动作描述 使用聚集索引 使用非聚集索引    

列经常被分组排序 应 应    

返回某范围内的数据 应 不应    

一个或极少不同值 不应 不应   

小数目的不同值 应 不应   

大数目的不同值 不应 应    

频繁更新的列 不应 应    

外键列 应 应    

主键列 应 应    

频繁修改索引列 不应 应    

事实上，我们可以通过前面聚集索引和非聚集索引的定义的例子来理解上表。如：返回某范围内的数据一项。比如您的某个表有一个时间列，恰好您把聚合索引建立在了该列，这时您查询2004年1月1日至2004年10月1日之间的全部数据时，这个速度就将是很快的，因为您的这本字典正文是按日期进行排序的，聚类索引只需要找到要检索的所有数据中的开头和结尾数据即可；而不像非聚集索引，必须先查到目录中查到每一项数据对应的页码，然后再根据页码查到具体内容。     

#### 结合实际，谈索引使用的误区    

理论的目的是应用。虽然我们刚才列出了何时应使用聚集索引或非聚集索引，但在实践中以上规则却很容易被忽视或不能根据实际情况进行综合分析。下面我们将根据在实践中遇到的实际问题来谈一下索引使用的误区，以便于大家掌握索引建立的方法。      

1、主键就是聚集索引     

这种想法笔者认为是极端错误的，是对聚集索引的一种浪费。虽然SQL SERVER默认是在主键上建立聚集索引的。      

通常，我们会在每个表中都建立一个ID列，以区分每条数据，并且这个ID列是自动增大的，步长一般为1。我们的这个办公自动化的实例中的列Gid就是如此。此时，如果我们将这个列设为主键，SQL SERVER会将此列默认为聚集索引。这样做有好处，就是可以让您的数据在数据库中按照ID进行物理排序，但笔者认为这样做意义不大。         

显而易见，聚集索引的优势是很明显的，而每个表中只能有一个聚集索引的规则，这使得聚集索引变得更加珍贵。          

从我们前面谈到的聚集索引的定义我们可以看出，使用聚集索引的最大好处就是能够根据查询要求，迅速缩小查询范围，避免全表扫描。在实际应用中，因为ID号是自动生成的，我们并不知道每条记录的ID号，所以我们很难在实践中用ID号来进行查询。这就使让ID号这个主键作为聚集索引成为一种资源浪费。其次，让每个ID号都不同的字段作为聚集索引也不符合“大数目的不同值情况下不应建立聚合索引”规则；当然，这种情况只是针对用户经常修改记录内容，特别是索引项的时候会负作用，但对于查询速度并没有影响。      

在办公自动化系统中，无论是系统首页显示的需要用户签收的文件、会议还是用户进行文件查询等任何情况下进行数据查询都离不开字段的是“日期”还有用户本身的“用户名”。     
 
通常，办公自动化的首页会显示每个用户尚未签收的文件或会议。虽然我们的where语句可以仅仅限制当前用户尚未签收的情况，但如果您的系统已建立了很长时间，并且数据量很大，那么，每次每个用户打开首页的时候都进行一次全表扫描，这样做意义是不大的，绝大多数的用户1个月前的文件都已经浏览过了，这样做只能徒增数据库的开销而已。事实上，我们完全可以让用户打开系统首页时，数据库仅仅查询这个用户近3个月来未阅览的文件，通过“日期”这个字段来限制表扫描，提高查询速度。如果您的办公自动化系统已经建立的2年，那么您的首页显示速度理论上将是原来速度8倍，甚至更快。       

2、只要建立索引就能显著提高查询速度      

事实上，我们可以发现上面的例子中，第2、3条语句完全相同，且建立索引的字段也相同；不同的仅是前者在fariqi字段上建立的是非聚合索引，后者在此字段上建立的是聚合索引，但查询速度却有着天壤之别。所以，并非是在任何字段上简单地建立索引就能提高查询速度。      

从建表的语句中，我们可以看到这个有着1000万数据的表中fariqi字段有5003个不同记录。在此字段上建立聚合索引是再合适不过了。在现实中，我们每天都会发几个文件，这几个文件的发文日期就相同，这完全符合建立聚集索引要求的：“既不能绝大多数都相同，又不能只有极少数相同”的规则。由此看来，我们建立“适当”的聚合索引对于我们提高查询速度是非常重要的。     

3、把所有需要提高查询速度的字段都加进聚集索引，以提高查询速度      

上面已经谈到：在进行数据查询时都离不开字段的是“日期”还有用户本身的“用户名”。既然这两个字段都是如此的重要，我们可以把他们合并起来，建立一个复合索引（compound index）。     

很多人认为只要把任何字段加进聚集索引，就能提高查询速度，也有人感到迷惑：如果把复合的聚集索引字段分开查询，那么查询速度会减慢吗？带着这个问题，我们来看一下以下的查询速度（结果集都是25万条数据）：（日期列fariqi首先排在复合聚集索引的起始列，用户名neibuyonghu排在后列）     

我们可以看到如果仅用聚集索引的起始列作为查询条件和同时用到复合聚集索引的全部列的查询速度是几乎一样的，甚至比用上全部的复合索引列还要略快（在查询结果集数目一样的情况下）；而如果仅用复合聚集索引的非起始列作为查询条件的话，这个索引是不起任何作用的。当然，语句1、2的查询速度一样是因为查询的条目数一样，如果复合索引的所有列都用上，而且查询结果少的话，这样就会形成“索引覆盖”，因而性能可以达到最优。同时，请记住：无论您是否经常使用聚合索引的其他列，但其前导列一定要是使用最频繁的列。     

#### 其他注意事项 

“水可载舟，亦可覆舟”，索引也一样。索引有助于提高检索性能，但过多或不当的索引也会导致系统低效。因为用户在表中每加进一个索引，数据库就要做更多的工作。过多的索引甚至会导致索引碎片。      

所以说，我们要建立一个“适当”的索引体系，特别是对聚合索引的创建，更应精益求精，以使您的数据库能得到高性能的发挥     


### 中间件
#### 什么是中间件
传统的架构模式就是 应用连接数据库直接对数据进行访问，这种架构特点就是简单方便。但是随着目前数据量不断的增大我们就遇到了问题:    
单个表数据量太大    
单个库数据量太大     
单台数据量服务器压力很大    
读写速度遇到瓶颈     
当面临以上问题时，我们会想到的第一种解决方式就是 向上扩展(scale up) 简单来说就是不断增加硬件性能。这种方式只能暂时解决问题，当业务量不断增长时还是解决不了问题。特别是淘宝，facebook，youtube这种业务成线性，甚至指数级上升的情况      
此时我们不得不依赖于第二种方式： 水平扩展 。 直接增加机器，把数据库放到不同服务器上，在应用到数据库之间加一个proxy进行路由，这样就可以解决上面的问题了。     

#### 中间件与读写分离
很多人都会把中间件认为是读写分离，其实读写分离只是中间件可以提供的一种功能，最主要的功能还是在于他可以分库分表 ，下面是一个读写分离的示意图
![image](https://github.com/miozeng/Review/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8Emysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/zjjimage1.jpg)


上面的图可以看出，红线代表写请求，绿线代表读请求。这就是一个简单的读写分离，下面我们在看看分库分表中间件。
![image](https://github.com/miozeng/Review/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8Emysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/zjjimage2.jpg)
上面这幅图就可以看出中间件作用，比如下面的这个SQL：

``` sql
select * from table_name where id = 1
``` 
按照中间件分库分表算法，此SQL将发送到DB1节点，由DB1这个MySQL负责解析和获取id=1的数据，并通过中间件返回给客户端。而在读写分离结构中并没有这些分库分表规则， 他只能在众多读节点中load balance随机进行分发，它要求各个节点都要存放一份完整的数据。

3.各类中间件比较
![image](https://github.com/miozeng/Review/blob/master/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8Emysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/zjjimage3.jpg)


分布式数据库中间件对比总结

Cobar:    
阿里巴巴B2B开发的关系型分布式系统，管理将近3000个MySQL实例。 在阿里经受住了考验，后面由于作者的走开的原因cobar没有人维护 了，阿里也开发了tddl替代cobar。     

MyCAT:       
社区爱好者在阿里cobar基础上进行二次开发，解决了cobar当时存 在的一些问题，并且加入了许多新的功能在其中。目前MyCAT社区活 跃度很高，目前已经有一些公司在使用MyCAT。总体来说支持度比 较高，也会一直维护下去，     

OneProxy:    
数据库界大牛，前支付宝数据库团队领导楼总开发，基于mysql官方 的proxy思想利用c进行开发的，OneProxy是一款商业收费的中间件， 楼总舍去了一些功能点，专注在性能和稳定性上。有朋友测试过说在 高并发下很稳定。     

Vitess:       
这个中间件是Youtube生产在使用的，但是架构很复杂。 与以往中间件不同，使用Vitess应用改动比较大要 使用他提供语言的API接口，我们可以借鉴他其中的一些设计思想。     

Kingshard:      
Kingshard是前360Atlas中间件开发团队的陈菲利用业务时间 用go语言开发的，目前参与开发的人员有3个左右， 目前来看还不是成熟可以使用的产品，需要在不断完善。      

Atlas:     
360团队基于mysql proxy 把lua用C改写。原有版本是支持分表， 目前已经放出了分库分表版本。在网上看到一些朋友经常说在高并 发下会经常挂掉，如果大家要使用需要提前做好测试。    

MaxScale与MySQL Route:     
这两个中间件都算是官方的吧，MaxScale是mariadb (MySQL原作者维护的一个版本)研发的，目前版本不支持分库分表。      
MySQL Route是现在MySQL 官方Oracle公司发布出来的一个中间件。       
http://blog.csdn.net/moonpure/article/details/52846447

### 高并发下insert、update、delete
1、拆分大的DELETE或INSERT语句       
如果你需要在一个在线的网站上去执行一个大的DELETE或INSERT查询，你需要非常小心，要避免你的操作让你的整个网站停止相应。因为这两个操作是会锁表的，表一锁住了，别的操作都进不来了。     
 
如果你把你的表锁上一段时间，比如30秒钟，那么对于一个有很高访问量的站点来说，这30秒所积累的访问进程/线程，数据库链接，打开的文件数，可能不仅仅会让你泊WEB服务Crash，还可能会让你的整台服务器马上掛了。      
所以，如果你有一个大的处理，你定你一定把其拆分，使用LIMIT条件是一个好的方法。下面是一个示例    
 
 
2、垂直分割   
“垂直分割”是一种把数据库中的表按列变成几张表的方法，这样可以降低表的复杂度和字段的数目，从而达到优化的目的。       
示例一：在Users表中有一个字段是家庭地址，这个字段是可选字段，相比起，而且你在数据库操作的时候除了个人信息外，你并不需要经常读取或是改写这个字段。那么，为什么不把他放到另外一张表中呢？这样会让你的表有更好的性能，大家想想是不是，大量的时候，我对于用户表来说，只有用户ID，用户名，口令，用户角色等会被经常使用。小一点的表总是会有好的性能。     
示例二：你有一个叫“last_login”的字段，它会在每次用户登录时被更新。但是，每次更新时会导致该表的查询缓存被清空。所以，你可以把这个字段放到另一个表中，这样就不会影响你对用户ID，用户名，用户角色的不停地读取了，因为查询缓存会帮你增加很多性能。     
另外，你需要注意的是，这些被分出去的字段所形成的表，你不会经常性地去Join他们，不然的话，这样的性能会比不分割时还要差，而且，会是极数级的下降   


#### INSERT

插入一个记录需要的时间由下列因素组成，其中的数字表示大约比例：    
连接：（3）     
发送查询给服务器：（2）     
分析查询：（2）     
插入记录：（1x记录大小）      
插入索引：（1x索引）       
关闭：（1）     
这不考虑打开表的初始开销，每个并发运行的查询打开。    
表的大小以logN （B树）的速度减慢索引的插入。      

加快插入的一些方法：     
如果同时从同一个客户端插入很多行，使用含多个VALUE的INSERT语句同时插入几行。这比使用单行INSERT语句快（在某些情况下快几倍）。如果你正向一个非空表添加数据，可以调节bulk_insert_buffer_size变量，使数据插入更快。     
如果你从不同的客户端插入很多行，能通过INSERT DELAYED语句加快速度。      
用MyISAM，如果在表中没有删除的行，能在SELECT语句正在运行的同时插入行。      
当从一个文本文件装载一个表时，使用LOAD DATA INFILE。这通常比使用很多INSERT语句快20倍。参见13.2.5节，“LOAD DATA INFILE语法”。     
当表有很多索引时，有可能要多做些工作使得LOAD DATA INFILE更快些。使用下列过程：    
有选择地用CREATE TABLE创建表。     
执行FLUSH TABLES语句或命令mysqladmin flush-tables。使用myisamchk –keys-used=0 -rq /path/to/db/tbl_name。这将从表中取消所有索引的使用。       

用LOAD DATA INFILE把数据插入到表中，因为不更新任何索引，因此很快。 如果只想在以后读取表，使用myisampack压缩它。           

用myisamchk -r -q /path/to/db/tbl_name重新创建索引。这将在写入磁盘前在内存中创建索引树，并且它更快，因为避免了大量磁盘搜索。结果索引树也被完美地平衡。 执行FLUSH TABLES语句或mysqladmin flush-tables命令。      

请注意如果插入一个空MyISAM表，LOAD DATA INFILE也可以执行前面的优化；主要不同处是可以让myisamchk为创建索引分配更多的临时内存，比执行LOAD DATA INFILE语句时为服务器重新创建索引分配得要多。

也可以使用ALTER TABLE tbl_name DISABLE KEYS代替myisamchk –keys-used=0 -rq/path/to/db/tbl_name，使用ALTER TABLE tbl_name ENABLE KEYS代替myisamchk -r -q/path/to/db/tbl_name。使用这种方式，还可以跳过FLUSH TABLES。       

锁定表可以加速用多个语句执行的INSERT操作：     

``` sql
LOCK TABLES a WRITE;
INSERT INTO a VALUES (1,23),(2,34),(4,33);
INSERT INTO a VALUES (8,26),(6,29);
UNLOCK TABLES;
```

这样性能会提高，因为索引缓存区仅在所有INSERT语句完成后刷新到磁盘上一次。一般有多少INSERT语句即有多少索引缓存区刷新。如果能用一个语句插入所有的行，就不需要锁定。     

对于事务表，应使用BEGIN和COMMIT代替LOCK TABLES来加快插入。    

锁定也将降低多连接测试的整体时间，尽管因为它们等候锁定最大等待时间将上升。例如：

MySQL
``` sql
Connection 1 does 1000 inserts 
Connections 2, 3, and 4 do 1 insert 
Connection 5 does 1000 inserts
```

如果不使用锁定，2、3和4将在1和5前完成。如果使用锁定，2、3和4将可能不在1或5前完成，但是整体时间应该快大约40%。

INSERT、UPDATE和DELETE操作在MySQL中是很快的，通过为在一行中多于大约5次连续不断地插入或更新的操作加锁，可以获得更好的整体性能。如果在一行中进行多次插入，可以执行LOCK TABLES，随后立即执行UNLOCK TABLES(大约每1000行)以允许其它的线程访问表。这也会获得好的性能。     

INSERT装载数据比LOAD DATA INFILE要慢得多，即使是使用上述的策略。      

为了对LOAD DATA INFILE和INSERT在MyISAM表得到更快的速度，通过增加key_buffer_size系统变量来扩大 键高速缓冲区。     

INSERT语法

``` sql


INSERT [LOW_PRIORITY | DELAYED | HIGH_PRIORITY] [IGNORE] 
[INTO] tbl_name [(col_name,...)] 
VALUES ({expr | DEFAULT},...),(...),... 
[ ON DUPLICATE KEY UPDATE col_name=expr, ... ]

或：

INSERT [LOW_PRIORITY | DELAYED | HIGH_PRIORITY] [IGNORE] 
[INTO] tbl_name 
SET col_name={expr | DEFAULT}, ... 
[ ON DUPLICATE KEY UPDATE col_name=expr, ... ]

或：

INSERT [LOW_PRIORITY | HIGH_PRIORITY] [IGNORE] 
[INTO] tbl_name [(col_name,...)] 
SELECT ... 
[ ON DUPLICATE KEY UPDATE col_name=expr, ... ]

一、DELAYED 的使用    

使用延迟插入操作DELAYED调节符应用于INSERT和REPLACE语句。当DELAYED插入操作到达的时候，服务器把数据行放入一个队列中，并立即给客户端返回一个状态信息，这样客户端就可以在数据表被真正地插入记录之前继续进行操作了。如果读取者从该数据表中读取数据，队列中的数据就会被保持着，直到没有读取者为止。      

接着服务器开始插入延迟数据行（delayed-row）队列中的数据行。在插入操作的同时，服务器还要检查是否有新的读取请求到达和等待。如果有，延迟数据行队列就被挂起，允许读取者继续操作。当没有读取者的时候，服务器再次开始插入延迟的数据行。这个过程一直进行，直到队列空了为止。    

几点要注意事项：    

INSERT DELAYED应该仅用于指定值清单的INSERT语句。服务器忽略用于INSERT DELAYED…SELECT语句的DELAYED。服务器忽略用于INSERT DELAYED…ON DUPLICATE UPDATE语句的DELAYED。     

因为在行被插入前，语句立刻返回，所以您不能使用LAST_INSERT_ID()来获取AUTO_INCREMENT值。AUTO_INCREMENT值可能由语句生成。

对于SELECT语句，DELAYED行不可见，直到这些行确实被插入了为止。

DELAYED在从属复制服务器中被忽略了，因为DELAYED不会在从属服务器中产生与主服务器不一样的数据。注意，目前在队列中的各行只保存在存储器中，直到它们被插入到表中为止。这意味着，如果您强行中止了mysqld(例如，使用kill -9)或者如果mysqld意外停止，则所有没有被写入磁盘的行都会丢失。     

二、IGNORE的使用     

IGNORE是MySQL相对于标准SQL的扩展。如果在新表中有重复关键字，或者当STRICT模式启动后出现警告，则使用IGNORE控制ALTER TABLE的运行。

如果没有指定IGNORE，当重复关键字错误发生时，复制操作被放弃，返回前一步骤。     

如果指定了IGNORE，则对于有重复关键字的行，只使用第一行，其它有冲突的行被删除。并且，对错误值进行修正，使之尽量接近正确值。insert ignore into tb(…) value(…)这样不用校验是否存在了，有则忽略，无则添加。      

三、ON DUPLICATE KEY UPDATE的使用     

如果您指定了ON DUPLICATE KEY UPDATE，并且插入行后会导致在一个UNIQUE索引或PRIMARY KEY中出现重复值，则执行旧行UPDATE。例如，如果列a被定义为UNIQUE，并且包含值1，则以下两个语句具有相同的效果：     

``` sql
mysql> INSERT INTO table (a,b,c) VALUES (1,2,3) 
-> ON DUPLICATE KEY UPDATE cc=c+1; 
mysql> UPDATE table SET cc=c+1 WHERE a=1;
```
如果行作为新记录被插入，则受影响行的值为1；如果原有的记录被更新，则受影响行的值为2。    

注释：如果列b也是唯一列，则INSERT与此UPDATE语句相当：   

mysql> UPDATE table SET cc=c+1 WHERE a=1 OR b=2 LIMIT 1;  



如果a=1 OR b=2与多个行向匹配，则只有一个行被更新。通常，您应该尽量避免对带有多个唯一关键字的表使用ON DUPLICATE KEY子句。您可以在UPDATE子句中使用VALUES(col_name)函数从INSERT…UPDATE语句的INSERT部分引用列值。换句话说，如果没有发生重复关键字冲突，则UPDATE子句中的VALUES(col_name)可以引用被插入的col_name的值。本函数特别适用于多行插入。VALUES()函数只在INSERT…UPDATE语句中有意义，其它时候会返回NULL。     

示例：     

``` sql
mysql> INSERT INTO table (a,b,c) VALUES (1,2,3),(4,5,6)

-> ON DUPLICATE KEY UPDATE c=VALUES(a)+VALUES(b);
```
本语句与以下两个语句作用相同：

``` sql
mysql> INSERT INTO table (a,b,c) VALUES (1,2,3) 
-> ON DUPLICATE KEY UPDATE c=3; 
mysql> INSERT INTO table (a,b,c) VALUES (4,5,6) 
-> ON DUPLICATE KEY UPDATE c=9;
``` 

当您使用ON DUPLICATE KEY UPDATE时，DELAYED选项被忽略。   



